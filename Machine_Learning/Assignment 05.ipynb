{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e99d460e",
   "metadata": {},
   "source": [
    "### 1. What are the key tasks that machine learning entails? What does data pre-processing imply?\n",
    "The key tasks involved in machine learning are data pre-processing, model selection, training, validation, and testing.\n",
    "\n",
    "Data pre-processing involves cleaning, transforming, and organizing raw data into a structured format that can be used by machine learning algorithms. This includes removing missing or irrelevant data, handling outliers, and converting data into numerical form.\n",
    "\n",
    "Model selection involves choosing the appropriate algorithm or model that is suitable for the given task and data type. This involves selecting the most appropriate features, hyperparameters, and optimization algorithms for the model.\n",
    "\n",
    "Training involves using the selected model and the pre-processed data to train the machine learning algorithm. The goal is to find the optimal parameters for the model, which will enable it to make accurate predictions on unseen data.\n",
    "\n",
    "Validation involves evaluating the performance of the trained model on a separate validation set of data to ensure that it generalizes well.\n",
    "\n",
    "Testing involves using the trained model to make predictions on a separate test set of data to evaluate its performance on unseen data.\n",
    "\n",
    "Data pre-processing is the initial step in machine learning, where raw data is transformed and cleaned to make it suitable for analysis. This is essential to ensure that the machine learning model is trained on clean and meaningful data that can produce accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaad3f2",
   "metadata": {},
   "source": [
    "### 2. Describe quantitative and qualitative data in depth. Make a distinction between the two.\n",
    "Quantitative data refers to numerical values or quantities that can be measured and analyzed using statistical methods. Examples include height, weight, age, and income. Quantitative data can be further categorized as discrete or continuous. Discrete data takes on a finite set of values, while continuous data can take on any value within a range.\n",
    "\n",
    "Qualitative data, on the other hand, refers to non-numerical information that cannot be easily quantified. It is typically descriptive in nature, and can be categorized into nominal, ordinal, interval, or ratio scales. Examples include gender, race, occupation, and level of education. Qualitative data is often analyzed using methods such as content analysis or grounded theory.\n",
    "\n",
    "The key difference between the two is that quantitative data can be easily measured and analyzed using statistical methods, while qualitative data requires more interpretive and exploratory approaches to analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100083aa",
   "metadata": {},
   "source": [
    "### 3. Create a basic data collection that includes some sample records. Have at least one attribute from each of the machine learning data types.\n",
    "data collection could include a mix of categorical, numerical, binary, and text data types with sample records. For example, a dataset for customer churn prediction could have attributes such as customer age (numerical), gender (categorical), subscription plan (categorical), customer satisfaction score (numerical), tenure (numerical), total usage (numerical), and feedback (text)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143e502a",
   "metadata": {},
   "source": [
    "### 4. What are the various causes of machine learning data issues? What are the ramifications?\n",
    "There are various causes of machine learning data issues such as missing data, inaccurate data, inconsistent data, and unbalanced data. These issues can lead to biased models, reduced accuracy, and incorrect predictions. It is essential to address these issues through data cleaning and pre-processing to ensure that the data used for training is accurate and reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f411790a",
   "metadata": {},
   "source": [
    "### 5. Demonstrate various approaches to categorical data exploration with appropriate examples.\n",
    "One approach to explore categorical data is to create frequency tables and bar charts, which display the frequency or proportion of each category. For example, a bar chart could show the number of customers in each age group. Another approach is to use contingency tables and chi-squared tests to examine relationships between categorical variables. For instance, a contingency table can show how many customers from each age group bought a particular product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c766f4d",
   "metadata": {},
   "source": [
    "### 6. How would the learning activity be affected if certain variables have missing values? Having said that, what can be done about it?\n",
    "Missing values can negatively impact the performance of a machine learning model as they can introduce bias and decrease accuracy. There are several ways to handle missing values, including imputation methods such as mean, median, or mode imputation, hot-deck imputation, and K-nearest neighbor imputation. Alternatively, missing values can be removed from the dataset entirely, but this approach should be used with caution as it can result in a loss of information. Another approach is to use algorithms specifically designed to handle missing values, such as XGBoost or CatBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8f90a5",
   "metadata": {},
   "source": [
    "### 7. Describe the various methods for dealing with missing data values in depth.\n",
    "1. `Deletion:` The simplest method involves deleting the entire record or column that contains a missing value.\n",
    "2. `Imputation:` In this method, the missing values are filled with a value computed from other available data points in the dataset.\n",
    "3. `Machine learning algorithms:` Certain machine learning algorithms can handle missing values directly, either by ignoring them or by imputing them during the training phase.\n",
    "4. `Model-based imputation:` This method involves building a model to predict missing values based on the other variables in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414d72d5",
   "metadata": {},
   "source": [
    "### 8. What are the various data pre-processing techniques? Explain dimensionality reduction and function selection in a few words.\n",
    "Data cleaning: This involves identifying and correcting any errors, inaccuracies or inconsistencies in the data.\n",
    "\n",
    "Data integration: This involves combining data from different sources into a single dataset.\n",
    "\n",
    "Data transformation: This involves converting data from one format to another to make it more suitable for analysis.\n",
    "\n",
    "Data reduction: This involves reducing the size of the dataset while retaining as much relevant information as possible.\n",
    "\n",
    "Feature scaling: This involves scaling the features of the dataset to ensure that they are on the same scale.\n",
    "\n",
    "Data normalization: This involves transforming the data so that it has a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "Dimensionality reduction: This involves reducing the number of features in the dataset while retaining as much relevant information as possible.\n",
    "\n",
    "Feature selection: This involves selecting the most relevant features from the dataset and discarding the rest.\n",
    "\n",
    "Dimensionality reduction is a technique used to reduce the number of features in a dataset while retaining most of its important information. It is commonly used to remove irrelevant or redundant features and improve the performance and efficiency of machine learning models. The most common methods of dimensionality reduction include Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), and t-SNE.\n",
    "\n",
    "Feature selection is another data pre-processing technique that involves selecting a subset of the most relevant features from a larger set of features. This technique can help improve model accuracy, reduce overfitting, and improve the performance and efficiency of machine learning models. Feature selection methods include filter methods, wrapper methods, and embedded methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edbd133",
   "metadata": {},
   "source": [
    "### 9. Make brief notes on of the following ?\n",
    "\n",
    "`i. What is the IQR? What criteria are used to assess it?\n",
    "\n",
    "ii. Describe the various components of a box plot in detail? When will the lower whisker\n",
    "surpass the upper whisker in length? How can box plots be used to identify outliers?`\n",
    "\n",
    "i. The interquartile range (IQR) is a measure of variability that represents the difference between the upper and lower quartiles. The IQR is calculated by subtracting the value of the first quartile from the third quartile. The IQR is used to identify the spread of the middle 50% of the data and to identify potential outliers based on the following criteria: outliers fall below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR.\n",
    "\n",
    "ii. Box plots have five components: the median, the first and third quartiles (Q1 and Q3), the lower and upper whiskers, and any outliers that are plotted as individual points. The lower whisker extends from the minimum value to Q1, while the upper whisker extends from Q3 to the maximum value. If the data is symmetric, the whiskers will be of equal length; otherwise, the whisker on one side will be longer than the other. Box plots can be used to identify outliers by plotting any data points outside of the whiskers as individual points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cafaece",
   "metadata": {},
   "source": [
    "### 10. Make brief notes on any two of the following:\n",
    "\n",
    "1. Data collected at regular intervals\n",
    "\n",
    "2. The gap between the quartiles\n",
    "\n",
    "3. Use a cross-tab\n",
    "\n",
    "`Data collected at regular intervals` refers to data that is collected or recorded at fixed time intervals. This type of data is also known as time-series data, and it can be used to observe patterns, trends, and changes over time.\n",
    "\n",
    "`The gap between the quartiles` is also known as the interquartile range (IQR). It is the difference between the third quartile (Q3) and the first quartile (Q1) of a dataset. The IQR is used to identify the spread of a dataset, and it is often used as a measure of variability or dispersion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bc0e13",
   "metadata": {},
   "source": [
    "### 11. Make a comparison between:\n",
    "\n",
    "1. Data with nominal and ordinal values\n",
    "\n",
    "2. Histogram and box plot\n",
    "\n",
    "3. The average and median\n",
    "\n",
    "1. `Nominal data` is categorical data with no inherent order, whereas `ordinal data` is categorical data with an inherent order.\n",
    "2. `Histograms` display the distribution of a continuous variable, while `box plots` display the distribution of a variable across quartiles.\n",
    "3. The `average` (mean) is the sum of all values divided by the total number of values, while the `median` is the middle value in a sorted dataset. The mean is sensitive to extreme values, while the median is resistant to them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4773540a",
   "metadata": {},
   "source": [
    "### 1. What is the concept of supervised learning? What is the significance of the name ?\n",
    "\n",
    "Supervised learning is a type of machine learning where a model is trained using labeled data. The term \"supervised\" refers to the fact that the model is guided by a supervisor or teacher who provides labeled data and feedback to help the model learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc1514e",
   "metadata": {},
   "source": [
    "### 2. In the hospital sector, offer an example of supervised learning.\n",
    "One example of supervised learning in the hospital sector is predicting patient outcomes based on their medical history and current symptoms, using a classification algorithm trained on labeled data of past patient outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff67462",
   "metadata": {},
   "source": [
    "### 3. Give three supervised learning examples.\n",
    "Linear Regression.\n",
    "Nearest Neighbor.\n",
    "Gaussian Naive Bayes.\n",
    "Decision Trees.\n",
    "Support Vector Machine (SVM)\n",
    "Random Forest.\n",
    "\n",
    "\n",
    "1. Image classification: \n",
    "2. Sentiment analysis: \n",
    "3. Credit risk assessment:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82da5041",
   "metadata": {},
   "source": [
    "### 4. In supervised learning, what are classification and regression?\n",
    "Classification and regression are two types of supervised learning tasks. Classification involves predicting a categorical output, while regression involves predicting a continuous numerical output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba531164",
   "metadata": {},
   "source": [
    "### 5. Give some popular classification algorithms as examples.\n",
    "Logistic Regression\n",
    "Decision Trees\n",
    "Random Forest\n",
    "Support Vector Machines (SVMs)\n",
    "K-Nearest Neighbors (KNN)\n",
    "Naive Bayes\n",
    "Gradient Boosting\n",
    "Neural Networks\n",
    "AdaBoost\n",
    "XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b57152a",
   "metadata": {},
   "source": [
    "### 6. Briefly describe the SVM model.\n",
    "Support Vector Machines (SVM) is a popular classification algorithm that aims to find the best decision boundary that separates the data into different classes with the largest margin possible. It works by mapping the input data into a high-dimensional feature space and finding the optimal hyperplane that separates the data into different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c3f0c6",
   "metadata": {},
   "source": [
    "### 7. In SVM, what is the cost of misclassification?\n",
    "In SVM, the cost of misclassification refers to the penalty or loss incurred when a data point is classified into the wrong class. This cost is determined by the value of the regularization parameter C, which controls the trade-off between maximizing the margin and minimizing the misclassification error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fe5455",
   "metadata": {},
   "source": [
    "### 8. In the SVM model, define Support Vectors.\n",
    "Support Vectors are the data points closest to the decision boundary in the SVM model. They are the most critical data points in determining the optimal hyperplane and defining the margin between the classes. The SVM algorithm uses only these support vectors to construct the decision boundary, making it memory-efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9aa37b",
   "metadata": {},
   "source": [
    "### 9. In the SVM model, define the kernel.\n",
    "In the SVM model, a kernel is a function that takes the input data and transforms it into a higher-dimensional feature space where it becomes separable by a hyperplane. Common kernel functions include linear, polynomial, radial basis function (RBF), and sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63906079",
   "metadata": {},
   "source": [
    "### 10. What are the factors that influence SVM's effectiveness?\n",
    "The effectiveness of SVM is influenced by several factors, including the choice of kernel function, kernel parameters, regularization parameter C, and the amount and quality of training data. SVM can also be sensitive to outliers and class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079908b3",
   "metadata": {},
   "source": [
    "### 11. What are the benefits of using the SVM model?\n",
    "\n",
    "The benefits of using the SVM model include its ability to handle high-dimensional and complex datasets, its versatility in different problem domains and applications, and its effectiveness in handling binary and multi-class classification tasks. SVM can also generalize well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304d7309",
   "metadata": {},
   "source": [
    "### 12. What are the drawbacks of using the SVM model?\n",
    "The drawbacks of using the SVM model include its sensitivity to hyperparameter tuning, its computational complexity and memory requirements, and its difficulty in handling large datasets. SVM can also be sensitive to the quality of training data and may not perform well with noisy or unbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2345ca7",
   "metadata": {},
   "source": [
    "### 13. Notes should be written on\n",
    "\n",
    "1. The kNN algorithm has a validation flaw.\n",
    "\n",
    "2. In the kNN algorithm, the k value is chosen.\n",
    "\n",
    "3. A decision tree with inductive bias\n",
    "\n",
    "The kNN algorithm has a validation flaw known as \"bleeding.\" This occurs when a data point is used in both the training and validation sets, leading to overly optimistic performance estimates.\n",
    "\n",
    "In the kNN algorithm, the k value represents the number of nearest neighbors used to make a classification decision. The optimal value of k depends on the problem domain and can be determined using cross-validation or other model selection techniques.\n",
    "\n",
    "A decision tree with inductive bias is a decision tree algorithm that incorporates prior knowledge or assumptions about the problem domain into the tree construction process. This bias can improve the accuracy and interpretability of the resulting tree. Common examples include C4.5 and ID3 algorithms, which use entropy-based metrics to select the best splitting criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42694248",
   "metadata": {},
   "source": [
    "### 14. What are some of the benefits of the kNN algorithm?\n",
    "Some benefits of the kNN algorithm include its simplicity, versatility, and ability to handle complex nonlinear decision boundaries. It also requires minimal training time and can handle both binary and multi-class classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118f55b7",
   "metadata": {},
   "source": [
    "### 15. What are some of the kNN algorithm's drawbacks?\n",
    "Some drawbacks of the kNN algorithm include its sensitivity to the choice of distance metric, the curse of dimensionality, and its high memory usage for large datasets. It can also be slow at runtime and may require scaling of input features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ec2e76",
   "metadata": {},
   "source": [
    "### 16. Explain the decision tree algorithm in a few words.\n",
    "A decision tree algorithm is a machine learning method that constructs a tree-like model of decisions and their possible consequences. It works by recursively splitting the dataset into subsets based on the feature that maximizes the information gain or minimizes the impurity at each node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40db5d1",
   "metadata": {},
   "source": [
    "### 17. What is the difference between a node and a leaf in a decision tree?\n",
    "In a decision tree, a node represents a feature or attribute used for splitting the dataset, while a leaf node represents a class label or a decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efe9ddd",
   "metadata": {},
   "source": [
    "### 18. What is a decision tree's entropy?\n",
    "In a decision tree, entropy is a measure of the impurity or uncertainty of the dataset at a given node. It is calculated as the sum of the negative logarithm of the class probabilities weighted by their occurrence frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea64a5bd",
   "metadata": {},
   "source": [
    "### 19. In a decision tree, define knowledge gain.\n",
    "In a decision tree, knowledge gain is a measure of the improvement in entropy or impurity achieved by splitting the dataset on a particular attribute. It is calculated as the difference between the entropy of the parent node and the weighted sum of the entropy of the child nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85916546",
   "metadata": {},
   "source": [
    "### 20. Choose three advantages of the decision tree approach and write them down.\n",
    "Three advantages of the decision tree approach are its interpretability, its ability to handle both categorical and numerical data, and its robustness to missing values and noisy data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dba46a",
   "metadata": {},
   "source": [
    "### 21. Make a list of three flaws in the decision tree process.\n",
    "Three flaws in the decision tree process are its tendency to overfit the training data, its sensitivity to the order of input features, and its bias towards features with many categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323a32c9",
   "metadata": {},
   "source": [
    "### 22. Briefly describe the random forest model.\n",
    "The random forest model is an ensemble learning method that combines multiple decision trees to improve accuracy and robustness. It works by randomly sampling the data and features for each tree and aggregating their predictions through voting or averaging. It can handle high-dimensional and noisy data and is less prone to overfitting than single decision trees."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

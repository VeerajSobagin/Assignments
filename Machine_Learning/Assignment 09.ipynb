{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae33f661",
   "metadata": {},
   "source": [
    "### 1. What is feature engineering, and how does it work? Explain the various aspects of feature engineering in depth.\n",
    "Feature engineering is the process of selecting and extracting relevant features from raw data to improve the performance of machine learning models. It involves identifying significant features, transforming data into numerical features, and selecting relevant features. Other aspects include encoding categorical features, scaling features, handling missing values, and creating new features using domain knowledge. Feature engineering requires a deep understanding of the data and domain expertise to develop effective features that improve the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18565c2",
   "metadata": {},
   "source": [
    "### 2. What is feature selection, and how does it work? What is the aim of it? What are the various methods of function selection?\n",
    "Feature selection is the process of choosing relevant and informative features to train a model while discarding the unnecessary or redundant ones. Its aim is to reduce the complexity of the model, improve its accuracy and generalization. The various methods of feature selection are Filter methods, Wrapper methods, and Embedded methods. Filter methods use statistical methods to rank features, Wrapper methods evaluate the performance of the model on different feature subsets, and Embedded methods select features while training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf2809d",
   "metadata": {},
   "source": [
    "### 3. Describe the function selection filter and wrapper approaches. State the pros and cons of each approach?\n",
    "The filter approach selects functions based on their inherent characteristics, such as correlation and mutual information with the target variable. The wrapper approach selects features by evaluating the performance of the model with a particular subset of features. Filter methods are faster and less prone to overfitting, but they may miss essential features. Wrapper methods are more thorough, but they can be time-consuming and overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c463bb",
   "metadata": {},
   "source": [
    "### 4. Please Answer the following Questions :\n",
    "\n",
    "`1. Describe the overall feature selection process.`\n",
    "\n",
    "`2. Explain the key underlying principle of feature extraction using an example. What are the most widely used function extraction algorithms?`\n",
    "\n",
    "The overall feature selection process involves selecting a subset of relevant features from a larger set of features to improve the performance of a machine learning model. This can be done by evaluating the relevance and redundancy of each feature using various methods such as correlation, mutual information, and statistical tests.\n",
    "\n",
    "The key underlying principle of feature extraction is to transform raw data into a feature space that is more suitable for machine learning algorithms. For example, in image processing, features such as edges, textures, and shapes can be extracted from images to represent the content of the image in a more compact and discriminative way. The most widely used feature extraction algorithms include Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), and Independent Component Analysis (ICA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df885913",
   "metadata": {},
   "source": [
    "### 5. Describe the feature engineering process in the sense of a text categorization issue.\n",
    "In text categorization, feature engineering involves transforming raw text data into a set of relevant features that can be used for machine learning algorithms. This includes techniques such as tokenization, stop-word removal, stemming/lemmatization, and feature scaling. For example, in sentiment analysis, the frequency of specific words or phrases in a document can be used as features to predict whether the sentiment is positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14b533c",
   "metadata": {},
   "source": [
    "### 6. What makes cosine similarity a good metric for text categorization? A document-term matrix has two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in cosine.\n",
    "\n",
    "Cosine similarity is a good metric for text categorization because it measures the similarity between two documents based on the angle between their feature vectors in high-dimensional space. It is robust to differences in document length and scale.\n",
    "\n",
    "To find the cosine similarity between the two rows, we first calculate the dot product of the two vectors:\n",
    "\n",
    "2*2 + 3*1 + 2*0 + 0*0 + 2*3 + 3*2 + 3*1 + 0*3 + 1*1 = 28\n",
    "\n",
    "Next, we calculate the Euclidean norms of each vector:\n",
    "\n",
    "sqrt(2^2 + 3^2 + 2^2 + 0^2 + 2^2 + 3^2 + 3^2 + 0^2 + 1^2) = sqrt(30) ≈ 5.48\n",
    "\n",
    "sqrt(2^2 + 1^2 + 0^2 + 0^2 + 3^2 + 2^2 + 1^2 + 3^2 + 1^2) = sqrt(23) ≈ 4.80\n",
    "\n",
    "Finally, we divide the dot product by the product of the Euclidean norms:\n",
    "\n",
    "cosine_similarity = 28 / (5.48 * 4.80) ≈ 0.981"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4110b58e",
   "metadata": {},
   "source": [
    "### 7. Explain the following:\n",
    "\n",
    "1. What is the formula for calculating Hamming distance? Between 10001011 and 11001111, calculate the Hamming gap.\n",
    "\n",
    "2. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0, 0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1).\n",
    "\n",
    "The Hamming distance is the number of positions where two strings of equal length differ. For example, between the binary strings 10001011 and 11001111, the Hamming distance is 2 + 1 + 0 + 0 + 1 + 1 + 0 + 0 = 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6ffcbc",
   "metadata": {},
   "source": [
    "### 8. State what is meant by \"high-dimensional data set\"? Could you offer a few real-life examples? What are the difficulties in using machine learning techniques on a data set with many dimensions? What can be done about it?\n",
    "High-dimensional data sets refer to data sets that have a large number of features or variables relative to the number of observations. A few real-life examples include gene expression data, image data, and social network data. The difficulties of using machine learning techniques on high-dimensional data sets include overfitting, difficulty in visualization, and computational complexity. Techniques such as dimensionality reduction and feature selection can be used to mitigate these challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ccd35d",
   "metadata": {},
   "source": [
    "### 9. Make a few quick notes on:\n",
    "\n",
    "1. PCA is an acronym for Personal Computer Analysis.\n",
    "\n",
    "2. Use of vectors\n",
    "\n",
    "3. Embedded technique\n",
    "\n",
    "PCA stands for Principal Component Analysis, a technique used in machine learning and statistics to reduce the dimensionality of high-dimensional data by transforming it into a lower-dimensional space.\n",
    "\n",
    "Vectors are mathematical objects that represent quantities with both magnitude and direction. In machine learning, they are commonly used to represent data points and mathematical transformations.\n",
    "\n",
    "Embedding is a technique used in natural language processing to represent words or phrases as vectors in a high-dimensional space, which can then be used as input to machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0d47ec",
   "metadata": {},
   "source": [
    "### 10. Make a comparison between:\n",
    "\n",
    "1. Sequential backward exclusion vs. sequential forward selection\n",
    "\n",
    "2. Function selection methods: filter vs. wrapper\n",
    "\n",
    "3. SMC vs. Jaccard coefficient\n",
    "\n",
    "Sequential backward exclusion and sequential forward selection are both feature selection methods. The former removes features one by one, while the latter adds them one by one.\n",
    "\n",
    "Filter and wrapper methods are both used for feature selection. Filter methods rely on statistics, while wrapper methods involve a machine learning algorithm.\n",
    "\n",
    "SMC and Jaccard coefficient are both similarity measures used in data analysis. SMC is based on the number of matching features, while Jaccard coefficient is based on the proportion of matching features over the total number of features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

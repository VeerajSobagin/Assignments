{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c8f39bd",
   "metadata": {},
   "source": [
    "### 1. What exactly is a feature? Give an example to illustrate your point.\n",
    "In the context of machine learning, a feature refers to an individual measurable property or attribute of a phenomenon that is used to create a model. For example, in an image recognition task, features could include color, texture, and shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d15b0ca",
   "metadata": {},
   "source": [
    "### 2. What are the various circumstances in which feature construction is required?\n",
    "Feature construction is required when the raw data is not sufficient for the machine learning model to learn the patterns effectively. It can be necessary when dealing with missing values, noisy data, or when the given features are not directly related to the target variable. Additionally, feature construction can be used to combine existing features, transform features, or create new ones to improve the model's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50eee5d",
   "metadata": {},
   "source": [
    "### 3. Describe how nominal variables are encoded.\n",
    "Nominal variables are categorical variables without any inherent ordering. They can be encoded using one-hot encoding, where each unique value of the variable is represented by a binary indicator variable (0 or 1) in a new column. For example, a nominal variable \"color\" with values \"red\", \"green\", and \"blue\" can be encoded as three binary variables: \"color_red\", \"color_green\", and \"color_blue\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a629419e",
   "metadata": {},
   "source": [
    "### 4. Describe how numeric features are converted to categorical features.\n",
    "To convert numeric features to categorical features, we first need to create bins or categories based on the range of values. This process is called binning. We can then assign each value to a corresponding category. For example, we can convert ages (numeric) into age groups (categorical) such as \"0-20\", \"21-40\", \"41-60\", and \"61 and above\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c36a65",
   "metadata": {},
   "source": [
    "### 5. Describe the feature selection wrapper approach. State the advantages and disadvantages of this approach?\n",
    "The feature selection wrapper approach involves employing a machine learning algorithm to assess the relevance of each feature. The algorithm is repeated multiple times for different feature subsets, and the best-performing subset is chosen. The main advantage of this approach is that it accounts for feature interactions, but it is computationally expensive and can be prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2282837d",
   "metadata": {},
   "source": [
    "### 6. When is a feature considered irrelevant? What can be said to quantify it?\n",
    "A feature is considered irrelevant if it does not contribute significantly to the model's predictive power or increases the model's complexity without improving accuracy. It can be quantified using statistical measures such as correlation, mutual information, or feature importance scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391b7dcd",
   "metadata": {},
   "source": [
    "### 7. When is a function considered redundant? What criteria are used to identify features that could be redundant?\n",
    "A function is considered redundant when it does not add new or valuable information to the predictive model. Correlation or mutual information is used to identify features that could be redundant. Features with high correlation or mutual information may be considered redundant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5e7740",
   "metadata": {},
   "source": [
    "### 8. What are the various distance measurements used to determine feature similarity?\n",
    "`Euclidean distance:` The straight-line distance between two points.\n",
    "`Manhattan distance:` The distance between two points measured along the axes of a Cartesian coordinate system.\n",
    "`Cosine similarity:` The angle between two vectors in a multi-dimensional space.\n",
    "`Jaccard similarity:` The ratio of the size of the intersection of two sets to the size of their union.\n",
    "`Hamming distance:` The number of positions at which the corresponding symbols are different between two strings.\n",
    "`Mahalanobis distance:` The distance between two points in a multi-dimensional space, taking into account the correlations among the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e6d500",
   "metadata": {},
   "source": [
    "### 9. State difference between Euclidean and Manhattan distances?\n",
    "Euclidean distance and Manhattan distance are both used to measure the distance between two data points. The main difference between them is the way distance is calculated. Euclidean distance is calculated as the square root of the sum of the squared differences between the corresponding coordinates of the two points. Manhattan distance is calculated as the sum of the absolute differences between the corresponding coordinates of the two points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908469b1",
   "metadata": {},
   "source": [
    "### 10. Distinguish between feature transformation and feature selection.\n",
    "Feature transformation involves creating new features by applying mathematical or statistical operations to the existing features, whereas feature selection is the process of selecting a subset of relevant features from the original feature set. Feature transformation involves changing the representation of the data, while feature selection involves selecting the most important features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01b837c",
   "metadata": {},
   "source": [
    "### 11. Make brief notes on any two of the following:\n",
    "\n",
    "1. SVD (Standard Variable Diameter Diameter)\n",
    "\n",
    "2. Collection of features using a hybrid approach\n",
    "\n",
    "3. The width of the silhouette\n",
    "\n",
    "4. Receiver operating characteristic curve\n",
    "\n",
    "\n",
    "The `silhouette width` is a measure of how well-separated clusters are in a clustering algorithm. A higher silhouette width indicates better-defined clusters.\n",
    "\n",
    "`Receiver operating characteristic (ROC) curve` is a graphical representation of the tradeoff between true positive and false positive rates for a binary classifier at different classification thresholds. It is often used to evaluate the performance of classification models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

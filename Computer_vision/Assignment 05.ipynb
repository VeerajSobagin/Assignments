{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "334f6fec",
   "metadata": {},
   "source": [
    "## 1. How can each of these parameters be fine-tuned? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debdc512",
   "metadata": {},
   "source": [
    "### • Number of hidden layers\n",
    "The number of hidden layers can be fine-tuned by experimenting with different values and assessing their impact on the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f073a3",
   "metadata": {},
   "source": [
    "### • Network architecture (network depth)\n",
    "Network depth can be adjusted by adding or removing layers and testing the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dfff8f",
   "metadata": {},
   "source": [
    "### • Each layer's number of neurons (layer width)\n",
    "The number of neurons in each layer can be tuned by experimentation to find the optimal number that balances model complexity and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46db4c6f",
   "metadata": {},
   "source": [
    "### • Form of activation\n",
    "Form of activation: Different activation functions can be used to determine the best fit for a specific problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7634840",
   "metadata": {},
   "source": [
    "### • Optimization and learning\n",
    "The choice of optimization algorithm and learning rate can be adjusted to improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d20f174",
   "metadata": {},
   "source": [
    "### • Learning rate and decay schedule\n",
    "Learning rate and decay schedule can be tuned to improve model convergence and prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c16a65",
   "metadata": {},
   "source": [
    "### • Mini batch size\n",
    "Mini-batch size is a hyperparameter that controls the number of samples processed in one iteration during model training. It affects the training speed, memory consumption, and convergence of the model. A small batch size leads to faster training but noisy gradients, while a large batch size gives smoother gradients but slower training. The optimal batch size depends on the available resources and the dataset size. Larger datasets generally require larger batch sizes, while smaller datasets can benefit from smaller batches. In general, a batch size of 32 or 64 is commonly used in deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f6641e",
   "metadata": {},
   "source": [
    "### • Algorithms for optimization\n",
    "Different optimization algorithms can be used to improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b37d8c7",
   "metadata": {},
   "source": [
    "### • The number of epochs (and early stopping criteria)\n",
    "The number of epochs: We can experiment with different epoch values and early stopping criteria to find the optimal number of epochs for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69906205",
   "metadata": {},
   "source": [
    "## • Overfitting that be avoided by using regularization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bc4cc1",
   "metadata": {},
   "source": [
    "### • L2 normalization\n",
    "L2 normalization is a regularization technique used to prevent overfitting in neural networks. It involves adding a regularization term to the loss function during training that penalizes the network for having large weights. The L2 regularization term is calculated as the sum of the squares of all the weights in the network multiplied by a hyperparameter lambda, which controls the strength of the regularization. This encourages the network to learn simpler and more generalizable features by penalizing complex and overfitted models. L2 normalization is particularly useful when the number of features is large compared to the number of training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb375535",
   "metadata": {},
   "source": [
    "### • Drop out layers\n",
    "Dropout is a regularization technique used in neural networks to reduce overfitting. In a dropout layer, a certain percentage of randomly selected neurons are ignored during training. This means that each neuron's contribution to the activation of downstream neurons is temporarily removed. The network must then adapt to the loss of these neurons by becoming more robust and distributed. Dropout layers help to prevent overfitting and improve generalization by forcing the network to learn more robust features that are useful in predicting the target variable on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e391133d",
   "metadata": {},
   "source": [
    "### • Data augmentation\n",
    "Data augmentation is a technique used to artificially increase the size of a training dataset by generating new samples from existing ones through various transformations such as rotating, flipping, scaling, cropping, or adding noise. By applying these transformations, the model can learn to recognize patterns and features that are invariant to the variations introduced by the transformations. Data augmentation can help prevent overfitting and improve the model's generalization ability, particularly when the original dataset is small or imbalanced."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

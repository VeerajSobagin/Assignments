{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e49739d4",
   "metadata": {},
   "source": [
    "### 1. What is the COVARIATE SHIFT Issue, and how does it affect you?\n",
    "Covariate shift is a problem that arises when the input distribution of a machine learning model changes between the training and testing phase. This can lead to poor performance on the test data, as the model has not learned to generalize to the new distribution. It can be addressed by using techniques such as batch normalization and data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416cfb1c",
   "metadata": {},
   "source": [
    "### 2. What is the process of BATCH NORMALIZATION?\n",
    "Batch normalization is a technique used to standardize the inputs to a neural network by normalizing the activations of each layer. This helps to reduce the effects of covariate shift and can improve the stability and speed of training. It involves calculating the mean and standard deviation of the activations for each mini-batch and then applying a normalization transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d4bf42",
   "metadata": {},
   "source": [
    "### 3. Using our own terms and diagrams, explain LENET ARCHITECTURE.\n",
    "LeNet is a convolutional neural network architecture that was introduced in the 1990s for handwritten digit recognition. It consists of two convolutional layers followed by two fully connected layers, with max pooling and tanh activation functions used throughout. The architecture is relatively simple compared to modern CNNs but was groundbreaking for its time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f97307",
   "metadata": {},
   "source": [
    "### 4. Using our own terms and diagrams, explain ALEXNET ARCHITECTURE.\n",
    "AlexNet is a deep convolutional neural network architecture that was introduced in 2012 and won the ImageNet Large Scale Visual Recognition Challenge. It consists of five convolutional layers followed by three fully connected layers, with max pooling and ReLU activation functions used throughout. Dropout and local response normalization were also used to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0611574",
   "metadata": {},
   "source": [
    "### 5. Describe the vanishing gradient problem.\n",
    "The vanishing gradient problem is a phenomenon that can occur during backpropagation, where gradients become very small as they propagate back through deep networks. This can lead to slow or unstable training, as the network is unable to learn from the gradients. The problem can be addressed by using techniques such as weight initialization and skip connections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c971c95f",
   "metadata": {},
   "source": [
    "### 6. What is NORMALIZATION OF LOCAL RESPONSE?\n",
    "Local response normalization (LRN) is a technique used in some CNN architectures to increase the selectivity of individual neurons. It involves normalizing the activity of a neuron with respect to its nearby neighbors, based on their Euclidean distance in feature space. This can help to promote competition between neurons and prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4da59f0",
   "metadata": {},
   "source": [
    "### 7. In AlexNet, what WEIGHT REGULARIZATION was used?\n",
    "AlexNet used L2 weight regularization to prevent overfitting. This involves adding a penalty term to the loss function that is proportional to the sum of the squares of the weights, encouraging the network to use smaller weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cbc695",
   "metadata": {},
   "source": [
    "### 8. Using our own terms and diagrams, explain VGGNET ARCHITECTURE.\n",
    "VGGNet is a deep convolutional neural network architecture that was introduced in 2014. It consists of a series of 3x3 convolutional layers followed by max pooling, with a total of 16-19 layers depending on the configuration. The architecture is relatively simple and uniform, with small filters used throughout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b827e0",
   "metadata": {},
   "source": [
    "### 9. Describe VGGNET CONFIGURATIONS.\n",
    "VGGNet comes in several configurations, including VGG16 and VGG19, which refer to the number of layers in the network. Each configuration consists of a series of convolutional layers with 3x3 filters, followed by max pooling and then several fully connected layers. ReLU activation functions are used throughout, and dropout and L2 weight regularization are used to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5f4c4c",
   "metadata": {},
   "source": [
    "### 10. What regularization methods are used in VGGNET to prevent overfitting?\n",
    "VGGNet uses dropout and L2 weight regularization to prevent overfitting. Dropout randomly drops out a fraction of the neurons during training, forcing the network to learn more robust features. L2 weight regularization adds a penalty term to the loss function that is proportional to the sum of the squares of the weights, encouraging the network to use smaller weights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

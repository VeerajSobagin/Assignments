{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11beb164",
   "metadata": {},
   "source": [
    "### 1. Explain the architecture of BERT\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based neural network architecture that utilizes a pre-training phase to learn contextual language representations. BERT consists of a stack of encoder layers, where each layer has a multi-head self-attention mechanism and a feedforward neural network. BERT has two variants: BERT base, which has 12 encoder layers, and BERT large, which has 24 encoder layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18255b51",
   "metadata": {},
   "source": [
    "### 2. Explain Masked Language Modeling (MLM)\n",
    "Masked Language Modeling (MLM) is a pre-training task in BERT where a random subset of input tokens is replaced with a special [MASK] token, and the objective is to predict the original token. This task helps the model learn bidirectional representations and understand the context of each token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67463906",
   "metadata": {},
   "source": [
    "### 3. Explain Next Sentence Prediction (NSP)\n",
    "Next Sentence Prediction (NSP) is another pre-training task in BERT where the model predicts whether two input sentences are consecutive in a document or not. This task helps the model understand the relationships between sentences and improve its ability to generate coherent text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a1ad79",
   "metadata": {},
   "source": [
    "### 4. What is Matthews evaluation?\n",
    "Matthews evaluation is a binary classification evaluation metric that calculates the Matthews Correlation Coefficient (MCC) between the predicted and true binary labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cd17cd",
   "metadata": {},
   "source": [
    "### 5. What is Matthews Correlation Coefficient (MCC)?\n",
    "Matthews Correlation Coefficient (MCC) is a measure of the quality of binary (two-class) classifications, ranging from -1 to +1. A coefficient of +1 represents a perfect prediction, 0 represents a random prediction, and -1 represents an inverse prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4578ac61",
   "metadata": {},
   "source": [
    "### 6. Explain Semantic Role Labeling\n",
    "Semantic Role Labeling (SRL) is a natural language processing task that involves identifying the semantic roles of phrases in a sentence, such as the agent, patient, and instrument. SRL is used in applications such as question answering, text summarization, and machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca440028",
   "metadata": {},
   "source": [
    "### 7. Why Fine-tuning a BERT model takes less time than pretraining\n",
    "Fine-tuning a BERT model takes less time than pre-training because the pre-trained model has already learned contextualized representations of words and phrases, which can be fine-tuned for downstream tasks with relatively little additional training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668ed794",
   "metadata": {},
   "source": [
    "### 8. Recognizing Textual Entailment (RTE)\n",
    "Recognizing Textual Entailment (RTE) is a natural language processing task that involves determining whether a given text T entails a hypothesis H, contradicts H, or is neutral with respect to H. RTE is used in applications such as question answering, text classification, and information retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7769d386",
   "metadata": {},
   "source": [
    "### 9. Explain the decoder stack of GPT models.\n",
    "The decoder stack of GPT models consists of multiple layers of decoder blocks, each of which has a multi-head self-attention mechanism and a feedforward neural network. The decoder blocks are similar to those in the encoder of a transformer-based architecture, but with a masked self-attention mechanism that allows the model to attend only to the left side of the sequence, ensuring that the model does not cheat by looking ahead when generating text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

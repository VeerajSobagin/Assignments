{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27dafdc4",
   "metadata": {},
   "source": [
    "### 1. What are Vanilla autoencoders\n",
    "Vanilla autoencoders are a type of neural network that is trained to encode and decode input data. The encoder takes the input data and compresses it into a lower dimensional representation, and the decoder reconstructs the original input from this compressed representation. The goal is to minimize the reconstruction error between the original input and the reconstructed output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1064b3fb",
   "metadata": {},
   "source": [
    "### 2. What are Sparse autoencoders\n",
    "Sparse autoencoders are similar to vanilla autoencoders, but with a constraint that encourages the encoder to learn a sparse representation of the input data. This can lead to better generalization and feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cd3a9c",
   "metadata": {},
   "source": [
    "### 3. What are Denoising autoencoders\n",
    "Denoising autoencoders are trained to reconstruct the original input from a noisy version of the input. The idea is to force the model to learn a more robust representation of the input data by exposing it to different types of noise during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b69b5d",
   "metadata": {},
   "source": [
    "### 4. What are Convolutional autoencoders\n",
    "Convolutional autoencoders use convolutional layers in both the encoder and decoder, making them well-suited for image data. They can learn local features and patterns that are translation-invariant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52579a68",
   "metadata": {},
   "source": [
    "### 5. What are Stacked autoencoders\n",
    "Stacked autoencoders are composed of multiple layers of autoencoders, where the output of one layer is used as the input to the next layer. This can lead to more powerful feature extraction and better reconstruction quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cc88bf",
   "metadata": {},
   "source": [
    "### 6. Explain how to generate sentences using LSTM autoencoders\n",
    "To generate sentences using LSTM autoencoders, the model is trained to encode and decode sequences of words. The decoder is initialized with a start token and generates the next word in the sequence by sampling from the output probability distribution of the model. This process is repeated until an end token is generated or a maximum sequence length is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55136248",
   "metadata": {},
   "source": [
    "### 7. Explain Extractive summarization\n",
    "Extractive summarization is a type of text summarization that involves selecting a subset of the most important sentences from the original text to create a summary. This is typically done using ranking algorithms that score each sentence based on its importance to the overall meaning of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ab0b3f",
   "metadata": {},
   "source": [
    "### 8. Explain Abstractive summarization\n",
    "Abstractive summarization is a more challenging type of text summarization that involves generating new sentences that capture the meaning of the original text. This requires a deeper understanding of the text and the ability to generate coherent and grammatically correct sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e16aa6",
   "metadata": {},
   "source": [
    "### 9. Explain Beam search\n",
    "Beam search is a decoding algorithm used in sequence-to-sequence models that involves keeping track of the k most likely output sequences at each decoding step, where k is a hyperparameter. The goal is to find the sequence with the highest probability according to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347b3830",
   "metadata": {},
   "source": [
    "### 10. Explain Length normalization\n",
    "Length normalization is a technique used to adjust the probability of a sequence based on its length. This is often done to prevent shorter sequences from being favored by the model during decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d878d4be",
   "metadata": {},
   "source": [
    "### 11. Explain Coverage normalization\n",
    "Coverage normalization is a technique used in abstractive summarization to encourage the model to generate diverse and non-repetitive summaries by penalizing the repetition of previously generated phrases or words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81212e6a",
   "metadata": {},
   "source": [
    "### 12. Explain ROUGE metric evaluation\n",
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics used to evaluate the quality of text summarization outputs by comparing them to reference summaries. The metrics measure the overlap between the generated summary and the reference summary in terms of n-gram matches and sentence-level recall."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

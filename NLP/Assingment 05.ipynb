{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c93b5133",
   "metadata": {},
   "source": [
    "### 1. What are Sequence-to-sequence models?\n",
    "Sequence-to-sequence (Seq2Seq) models are a type of neural network architecture designed to map input sequences to output sequences. They are commonly used for tasks such as machine translation, speech recognition, and image captioning. Seq2Seq models consist of an encoder that maps the input sequence to a fixed-length vector, and a decoder that generates the output sequence based on the encoded vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca7a333",
   "metadata": {},
   "source": [
    "### 2. What are the Problem with Vanilla RNNs?\n",
    "The problem with Vanilla RNNs is that they suffer from vanishing and exploding gradients, which can make it difficult to train the model effectively. Vanishing gradients occur when the gradients become very small, which can prevent the model from learning long-term dependencies. Exploding gradients occur when the gradients become very large, which can cause the model to diverge during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e178fe95",
   "metadata": {},
   "source": [
    "### 3. What is Gradient clipping?\n",
    "Gradient clipping is a technique used to prevent exploding gradients during training. It involves scaling the gradients if they exceed a certain threshold. This can help to prevent the model from diverging during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92be7e03",
   "metadata": {},
   "source": [
    "### 4. Explain Attention mechanism\n",
    "Attention mechanism is a technique used in neural networks to focus on specific parts of the input sequence when generating the output sequence. It allows the model to selectively weight different parts of the input sequence based on their relevance to the current output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105b333a",
   "metadata": {},
   "source": [
    "### 5. Explain Conditional random fields (CRFs)\n",
    "Conditional random fields (CRFs) are a type of probabilistic graphical model commonly used in natural language processing tasks such as named entity recognition and part-of-speech tagging. CRFs allow for modeling dependencies between neighboring output labels, which can improve the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55a3015",
   "metadata": {},
   "source": [
    "### 6. Explain self-attention\n",
    "Self-attention is a mechanism used in neural networks to compute the attention weights based on the current input sequence rather than a separate context vector. It allows the model to attend to different parts of the input sequence based on their relevance to the current output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1604eaa",
   "metadata": {},
   "source": [
    "### 7. What is Bahdanau Attention?\n",
    "Bahdanau Attention is a specific type of attention mechanism used in Seq2Seq models. It uses a separate context vector that is updated at each time step based on the current input and hidden state, which allows the model to attend to different parts of the input sequence at different time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75330fff",
   "metadata": {},
   "source": [
    "### 8. What is a Language Model?\n",
    "A language model is a type of neural network that is trained to predict the probability of a sequence of words. Language models are commonly used in natural language processing tasks such as speech recognition, machine translation, and text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84be7ff3",
   "metadata": {},
   "source": [
    "### 9. What is Multi-Head Attention?\n",
    "Multi-Head Attention is a variant of attention mechanism that allows the model to attend to different parts of the input sequence using multiple attention heads. This can improve the accuracy of the model by allowing it to capture multiple aspects of the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4d4b7a",
   "metadata": {},
   "source": [
    "### 10. What is Bilingual Evaluation Understudy (BLEU)\n",
    "Bilingual Evaluation Understudy (BLEU) is a metric used to evaluate the performance of machine translation systems. It compares the output of the machine translation system to one or more human translations using n-gram overlap and a brevity penalty to account for differences in output length."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

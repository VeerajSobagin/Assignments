{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0832f66",
   "metadata": {},
   "source": [
    "#### 1. What is the difference between a neuron and a neural network?\n",
    "\n",
    "A neuron is a basic computational unit in a neural network that receives input, performs a computation, and produces an output. A neural network, on the other hand, is a collection of interconnected neurons organized in layers, which allows for complex computations and learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98585b1e",
   "metadata": {},
   "source": [
    "#### 2. Can you explain the structure and components of a neuron?\n",
    "\n",
    "A neuron consists of three main components: inputs, weights, and an activation function. The inputs are numerical values representing the information from the previous layer or input data. Each input is associated with a weight that determines its contribution to the neuron's output. The weighted inputs are summed, and the resulting value is passed through an activation function, which introduces non-linearity and determines the neuron's output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f53350f",
   "metadata": {},
   "source": [
    "#### 3. Describe the architecture and functioning of a perceptron.\n",
    "\n",
    "A perceptron is the simplest form of an artificial neural network, consisting of a single layer of neurons. It takes multiple inputs, each with an associated weight, and applies the weighted sum to an activation function. The output of the perceptron is a binary decision based on the activation function's result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bd98ce",
   "metadata": {},
   "source": [
    "#### 4. What is the main difference between a perceptron and a multilayer perceptron?\n",
    "\n",
    "The main difference between a perceptron and a multilayer perceptron (MLP) is the number of layers. A perceptron has only one layer, while an MLP has multiple hidden layers between the input and output layers. This allows an MLP to learn more complex patterns and relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768ec558",
   "metadata": {},
   "source": [
    "#### 5. Explain the concept of forward propagation in a neural network.\n",
    "\n",
    "Forward propagation is the process of computing the output of a neural network given input data. It involves passing the input through each layer, where the weighted inputs are computed, transformed by activation functions, and passed as inputs to the next layer. This process continues until the final layer produces the network's output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e0bcb6",
   "metadata": {},
   "source": [
    "#### 6. What is backpropagation, and why is it important in neural network training?\n",
    "\n",
    "Backpropagation is the primary algorithm used to train neural networks. It involves propagating the error from the network's output back through the layers to adjust the weights and biases. By iteratively updating the weights based on the error, the network learns to make better predictions. Backpropagation is important because it enables the network to adjust its internal parameters and improve its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad536e3d",
   "metadata": {},
   "source": [
    "#### 7. How does the chain rule relate to backpropagation in neural networks?\n",
    "\n",
    "The chain rule is a fundamental mathematical concept that allows for the calculation of the derivative of a composition of functions. In the context of neural networks, the chain rule is used in backpropagation to compute the gradients of the network's parameters with respect to the loss function. It enables the efficient calculation of the gradients and the subsequent updates to the network's weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f88095",
   "metadata": {},
   "source": [
    "#### 8. What are loss functions, and what role do they play in neural networks?\n",
    "\n",
    "Loss functions, also known as cost functions or objective functions, quantify the discrepancy between the predicted output of a neural network and the true output. They serve as a measure of how well the network is performing. The role of loss functions in neural networks is to provide a quantitative measure that can be minimized during the training process, guiding the network's learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c88950",
   "metadata": {},
   "source": [
    "#### 9. Can you give examples of different types of loss functions used in neural networks?\n",
    "\n",
    "Examples of loss functions used in neural networks include mean squared error (MSE) for regression problems, binary cross-entropy for binary classification problems, and categorical cross-entropy for multi-class classification problems. Other loss functions such as hinge loss and log loss can be used for specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d048505f",
   "metadata": {},
   "source": [
    "#### 10. Discuss the purpose and functioning of optimizers in neural networks.\n",
    "\n",
    "Optimizers are algorithms used to adjust the weights and biases of a neural network based on the gradients computed during backpropagation. They determine how the network learns and updates its parameters to minimize the loss function. Optimizers utilize techniques such as gradient descent and its variants to iteratively update the network's parameters towards the optimal values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603ed2ba",
   "metadata": {},
   "source": [
    "#### 11. What is the exploding gradient problem, and how can it be mitigated?\n",
    "\n",
    "The exploding gradient problem occurs when the gradients in a neural network become very large during training, causing the network's weights to update excessively and destabilizing the learning process. To mitigate this issue, gradient clipping can be applied to limit the magnitude of the gradients, ensuring stable learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4745e3ec",
   "metadata": {},
   "source": [
    "#### 12. Explain the concept of the vanishing gradient problem and its impact on neural network training.\n",
    "\n",
    "The vanishing gradient problem refers to the situation where the gradients in a neural network become extremely small during backpropagation, making it difficult for the network to learn and update the early layers' weights effectively. This problem is particularly prominent in deep neural networks. Techniques such as using activation functions that alleviate the vanishing gradient problem, such as ReLU, and using skip connections, such as in residual networks, can mitigate this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3710e064",
   "metadata": {},
   "source": [
    "#### 13. How does regularization help in preventing overfitting in neural networks?\n",
    "\n",
    "Regularization techniques, such as L1 and L2 regularization, are used in neural networks to prevent overfitting. They introduce additional terms in the loss function that penalize large weights, encouraging the network to learn more robust and generalizable representations. By controlling the complexity of the network, regularization helps prevent overfitting and improve generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8506487e",
   "metadata": {},
   "source": [
    "#### 14. Describe the concept of normalization in the context of neural networks.\n",
    "\n",
    "Normalization in the context of neural networks refers to the process of scaling the input data to have a standard range or distribution. It helps ensure that features with different scales or units contribute equally to the learning process. Common normalization techniques include min-max scaling, z-score normalization, and batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb5aac5",
   "metadata": {},
   "source": [
    "#### 15. What are the commonly used activation functions in neural networks?\n",
    "\n",
    "Commonly used activation functions in neural networks include sigmoid, tanh, and ReLU (Rectified Linear Unit). Sigmoid and tanh functions introduce non-linearity and squash the output within a specific range, while ReLU provides a non-linear activation by thresholding the output at zero. Other activation functions like softmax and Leaky ReLU are used for specific purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11a3689",
   "metadata": {},
   "source": [
    "#### 16. Explain the concept of batch normalization and its advantages.\n",
    "\n",
    "Batch normalization is a technique used to normalize the activations of a neural network's hidden layers. It normalizes the inputs to a layer by subtracting the mean and dividing by the standard deviation, ensuring that the data has zero mean and unit variance. Batch normalization helps mitigate the problem of internal covariate shift and improves the stability and speed of training. It also acts as a regularizer, reducing the reliance on dropout or weight decay."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f717e02a",
   "metadata": {},
   "source": [
    "#### 17. Discuss the concept of weight initialization in neural networks and its importance.\n",
    "\n",
    "Weight initialization refers to the process of assigning initial values to the weights of a neural network before training. Proper weight initialization is crucial as it can affect the convergence and performance of the network. Techniques like Xavier initialization and He initialization aim to set the initial weights to suitable values, taking into account the number of inputs and outputs of the layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada9b360",
   "metadata": {},
   "source": [
    "#### 18. Can you explain the role of momentum in optimization algorithms for neural networks?\n",
    "\n",
    "Momentum in optimization algorithms for neural networks enhances the optimization process by introducing a velocity term. It allows the optimization algorithm to accumulate past gradients and smooth out the parameter updates. This helps to speed up convergence and navigate through flat or noisy regions of the optimization landscape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2296bc",
   "metadata": {},
   "source": [
    "#### 19. What is the difference between L1 and L2 regularization in neural networks?\n",
    "\n",
    "L1 and L2 regularization are techniques used to prevent overfitting in neural networks by adding penalty terms to the loss function. L1 regularization imposes a penalty proportional to the absolute value of the weights, encouraging sparsity and feature selection. L2 regularization imposes a penalty proportional to the squared value of the weights, encouraging smaller weights and more robust models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db4fc30",
   "metadata": {},
   "source": [
    "#### 20. How can early stopping be used as a regularization technique in neural networks?\n",
    "\n",
    "Early stopping is a regularization technique used in neural networks to prevent overfitting. It involves monitoring the performance of the model on a validation set during training and stopping the training process when the performance starts to deteriorate. Early stopping helps find the optimal balance between training the model sufficiently and avoiding overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b09dedd",
   "metadata": {},
   "source": [
    "#### 21. Describe the concept and application of dropout regularization in neural networks.\n",
    "\n",
    "Dropout regularization is a technique used in neural networks to prevent overfitting by randomly dropping out a proportion of the neurons during training. This forces the network to learn redundant representations and improves its generalization capability. Dropout helps reduce interdependent learning among neurons and encourages the network to be more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae504229",
   "metadata": {},
   "source": [
    "#### 22. Explain the importance of learning rate in training neural networks.\n",
    "\n",
    "The learning rate in training neural networks determines the step size at which the optimizer updates the model's weights. It plays a critical role in training as it affects the convergence speed and the ability to find the optimal solution. Choosing an appropriate learning rate is important to ensure the model learns effectively without converging too slowly or oscillating around the optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33ee27f",
   "metadata": {},
   "source": [
    "#### 23. What are the challenges associated with training deep neural networks?\n",
    "\n",
    "Training deep neural networks can be challenging due to issues such as vanishing or exploding gradients, overfitting, and increased computational requirements. Deep networks are prone to overfitting as they have many parameters, and training them requires large amounts of labeled data and computational resources. Techniques like regularization, careful weight initialization, and transfer learning can help mitigate these challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c250363",
   "metadata": {},
   "source": [
    "#### 24. How does a convolutional neural network (CNN) differ from a regular neural network?\n",
    "\n",
    "Convolutional neural networks (CNNs) differ from regular neural networks in their architecture and connectivity patterns. CNNs are designed specifically for processing grid-like data, such as images, by utilizing convolutional layers that apply filters to capture local patterns. They also include pooling layers to downsample the spatial dimensions and fully connected layers for classification or regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb53a96",
   "metadata": {},
   "source": [
    "#### 25. Can you explain the purpose and functioning of pooling layers in CNNs?\n",
    "\n",
    "Pooling layers in CNNs reduce the spatial dimensions of the input by aggregating or summarizing the information in local regions. Common pooling operations include max pooling, which selects the maximum value in each region, and average pooling, which calculates the average value. Pooling helps in extracting robust features, reducing computation, and providing translation invariance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3770b065",
   "metadata": {},
   "source": [
    "#### 26. What is a recurrent neural network (RNN), and what are its applications?\n",
    "\n",
    "Recurrent neural networks (RNNs) are a type of neural network specifically designed for processing sequential data. They have feedback connections that allow information to persist across time steps, enabling them to capture temporal dependencies. RNNs are widely used in applications such as natural language processing, speech recognition, and time series analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05369d58",
   "metadata": {},
   "source": [
    "#### 27. Describe the concept and benefits of long short-term memory (LSTM) networks.\n",
    "\n",
    "Long short-term memory (LSTM) networks are a type of RNN that address the vanishing gradient problem and can capture long-term dependencies in sequential data. LSTMs introduce memory cells and gating mechanisms that control the flow of information, enabling the network to selectively remember or forget information over time. LSTMs have been successful in tasks involving longer sequences or contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1284af",
   "metadata": {},
   "source": [
    "#### 28. What are generative adversarial networks (GANs), and how do they work?\n",
    "\n",
    "Generative adversarial networks (GANs) are a class of neural networks consisting of two components: a generator network and a discriminator network. GANs are trained in a competitive setting where the generator tries to produce realistic data samples, while the discriminator tries to distinguish between real and generated samples. GANs have been used for tasks such as generating realistic images and data synthesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c716f2d7",
   "metadata": {},
   "source": [
    "#### 29. Can you explain the purpose and functioning of autoencoder neural networks?\n",
    "\n",
    "Autoencoder neural networks are unsupervised learning models used for data compression and feature extraction. They consist of an encoder network that maps the input data to a lower-dimensional latent space and a decoder network that reconstructs the original data from the latent representation. Autoencoders are useful for tasks such as dimensionality reduction and anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261e6fd3",
   "metadata": {},
   "source": [
    "#### 30. Discuss the concept and applications of self-organizing maps (SOMs) in neural networks.\n",
    "\n",
    "Self-organizing maps (SOMs), also known as Kohonen maps, are unsupervised learning models used for visualizing and clustering high-dimensional data. SOMs create a low-dimensional grid of neurons that self-organize to represent the input data's underlying structure. They are useful for tasks such as data exploration, visualization, and finding similarities or patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeed26b",
   "metadata": {},
   "source": [
    "#### 31. How can neural networks be used for regression tasks?\n",
    "\n",
    "Neural networks can be used for regression tasks by modifying the output layer to produce continuous predictions rather than class labels. The loss function used for regression can be mean squared error (MSE) or other regression-specific loss functions. The network is trained to minimize the difference between its predictions and the true continuous target values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12ba601",
   "metadata": {},
   "source": [
    "#### 32. What are the challenges in training neural networks with large datasets?\n",
    "\n",
    "Training neural networks with large datasets can be challenging due to memory limitations and computational requirements. Techniques like mini-batch training, data augmentation, and distributed training can help address these challenges. Mini-batch training involves dividing the dataset into smaller batches and updating the weights incrementally. Data augmentation artificially increases the dataset size by applying transformations or adding noise to the training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1449df",
   "metadata": {},
   "source": [
    "#### 33. Explain the concept of transfer learning in neural networks and its benefits.\n",
    "\n",
    "Transfer learning is a technique in which pre-trained neural network models trained on one task are used as a starting point for another related task. By leveraging the knowledge learned from a source task, transfer learning helps improve the performance of a target task, especially when the target task has limited labeled data. It allows the network to benefit from features learned on a larger or similar dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc761f9",
   "metadata": {},
   "source": [
    "#### 34. How can neural networks be used for anomaly detection tasks?\n",
    "\n",
    "Neural networks can be used for anomaly detection tasks by training them on normal data samples and identifying deviations from the learned patterns. Unsupervised techniques such as autoencoders or generative models can be used to model normal behavior and detect anomalies as deviations from the learned representation. Neural networks can capture complex patterns and dependencies, making them suitable for anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ba229c",
   "metadata": {},
   "source": [
    "#### 35. Discuss the concept of model interpretability in neural networks.\n",
    "\n",
    "Model interpretability in neural networks refers to the ability to understand and explain how the model makes predictions. It is essential for gaining insights into the model's decision-making process, identifying the most influential features, and building trust in the model's predictions. Techniques like feature importance analysis, visualization, and interpretability methods such as SHAP values and LIME can aid in interpreting neural network models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1469425",
   "metadata": {},
   "source": [
    "#### 36. What are the advantages and disadvantages of deep learning compared to traditional machine learning algorithms?\n",
    "\n",
    "The advantages of deep learning compared to traditional machine learning algorithms include the ability to learn complex patterns and representations from large amounts of data, handling unstructured data like images and text, and achieving state-of-the-art performance in various domains. However, deep learning requires large amounts of labeled data, computational resources, and careful training to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a8f58c",
   "metadata": {},
   "source": [
    "#### 37. Can you explain the concept of ensemble learning in the context of neural networks?\n",
    "\n",
    "Ensemble learning in the context of neural networks involves combining the predictions of multiple neural network models to improve overall performance. Techniques like bagging, boosting, and stacking can be applied to create ensembles of neural networks, leveraging the diversity of models to improve accuracy, reduce overfitting, and increase robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1819eb11",
   "metadata": {},
   "source": [
    "#### 38. How can neural networks be used for natural language processing (NLP) tasks?\n",
    "\n",
    "Neural networks have been successfully applied to natural language processing (NLP) tasks such as text classification, sentiment analysis, machine translation, and language generation. Recurrent neural networks (RNNs), especially with LSTM or GRU (Gated Recurrent Unit) cells, are commonly used to model sequential data in NLP tasks. Transformers, a type of neural network architecture, have also gained popularity for tasks involving sequence-to-sequence learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71038861",
   "metadata": {},
   "source": [
    "#### 39. Discuss the concept and applications of self-supervised learning in neural networks.\n",
    "\n",
    "Self-supervised learning is a learning paradigm in which a model learns to predict certain aspects of the input data without explicit human annotations. It leverages the inherent structure or patterns in the data to create supervisory signals for training. Self-supervised learning has been successful in pre-training models on large datasets and transfer learning to downstream tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d42e767",
   "metadata": {},
   "source": [
    "#### 40. What are the challenges in training neural networks with imbalanced datasets?\n",
    "\n",
    "Training neural networks with imbalanced datasets can be challenging as the network may be biased towards the majority class. Techniques such as oversampling the minority class, undersampling the majority class, or using class weights during training can help address this issue. Evaluation metrics that consider the class imbalance, such as precision, recall, and F1-score, should also be used to assess model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77443235",
   "metadata": {},
   "source": [
    "#### 41. Explain the concept of adversarial attacks on neural networks and methods to mitigate them.\n",
    "\n",
    "Adversarial attacks on neural networks involve manipulating input data to deceive the model's predictions. Techniques like adding imperceptible perturbations to input samples or crafting adversarial examples can fool the model and lead to incorrect predictions. Adversarial training, input preprocessing techniques, and robust architectures can be used to mitigate the impact of such attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eb45e3",
   "metadata": {},
   "source": [
    "#### 42. Can you discuss the trade-off between model complexity and generalization performance in neural networks?\n",
    "\n",
    "The trade-off between model complexity and generalization performance in neural networks refers to the balance between the network's capacity to learn complex patterns in the training data and its ability to generalize well to unseen data. A model that is too simple may underfit and fail to capture important patterns, while a model that is too complex may overfit and memorize noise in the training data. Techniques such as regularization, early stopping, and model selection based on validation performance can help find the appropriate trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a82e5c",
   "metadata": {},
   "source": [
    "#### 43. What are some techniques for handling missing data in neural networks?\n",
    "\n",
    "Missing data in neural networks can be handled by techniques such as imputation, where missing values are estimated or replaced based on observed data. Other approaches include using specific network architectures that can handle missing values, applying data augmentation methods, or excluding samples with missing data during training. The choice of technique depends on the nature and amount of missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbd2faf",
   "metadata": {},
   "source": [
    "#### 44. Explain the concept and benefits of interpretability techniques like SHAP values and LIME in neural networks.\n",
    "\n",
    "Interpretability techniques like SHAP (SHapley Additive exPlanations) values and LIME (Local Interpretable Model-Agnostic Explanations) can be used in neural networks to provide insights into the contribution of each input feature to the model's predictions. SHAP values quantify the importance of each feature based on game theory, while LIME provides local explanations by approximating the model's behavior around specific instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7696707",
   "metadata": {},
   "source": [
    "#### 45. How can neural networks be deployed on edge devices for real-time inference?\n",
    "\n",
    "Deploying neural networks on edge devices for real-time inference involves optimizing the model's size, complexity, and computational requirements to run efficiently on resource-constrained devices. Techniques such as model compression, quantization, and hardware acceleration can be employed to reduce memory footprint and computational overhead, enabling efficient deployment on edge devices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613875fb",
   "metadata": {},
   "source": [
    "#### 46. Discuss the considerations and challenges in scaling neural network training on distributed systems.\n",
    "\n",
    "Scaling neural network training on distributed systems involves training models on multiple machines or devices in parallel to handle larger datasets and reduce training time. Challenges include efficient data parallelism, synchronization, and communication overhead. Techniques like model parallelism, parameter servers, and distributed optimization algorithms can be used to address these challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45909621",
   "metadata": {},
   "source": [
    "#### 47. What are the ethical implications of using neural networks in decision-making systems?\n",
    "\n",
    "The ethical implications of using neural networks in decision-making systems relate to issues such as fairness, bias, transparency, and accountability. Neural networks can learn biases present in the training data, leading to biased decisions. Understanding and addressing biases, ensuring transparency in decision-making, and having mechanisms for accountability and oversight are crucial in using neural networks responsibly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232a63f0",
   "metadata": {},
   "source": [
    "#### 48. Can you explain the concept and applications of reinforcement learning in neural networks?\n",
    "\n",
    "Reinforcement learning is a machine learning paradigm in which an agent learns to interact with an environment and maximize a reward signal. Neural networks can be used as function approximators in reinforcement learning to learn the policy or value function. Reinforcement learning has applications in areas such as robotics, game playing, and autonomous systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae59723",
   "metadata": {},
   "source": [
    "#### 49. Discuss the impact of batch size in training neural networks.\n",
    "\n",
    "The batch size in neural networks refers to the number of training samples processed together before updating the model's parameters. It impacts the trade-off between computation efficiency and model convergence. Larger batch sizes tend to provide faster computation but may reduce the model's ability to generalize well. Smaller batch sizes require more iterations but can help models converge to better solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009bb398",
   "metadata": {},
   "source": [
    "#### 50. What are the current limitations of neural networks and areas for future research?#### \n",
    "\n",
    "The limitations of neural networks include the need for large labeled datasets for training, high computational requirements, difficulties in interpreting complex models, vulnerability to adversarial attacks, and challenges in handling unbalanced or missing data. Areas for future research include improving interpretability, addressing ethical considerations, developing techniques for handling limited labeled data, and enhancing robustness and explainability in deep learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

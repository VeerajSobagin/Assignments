{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f99dca5e",
   "metadata": {},
   "source": [
    "### 1. What are the main tasks that autoencoders are used for?\n",
    "Autoencoders are mainly used for data compression, denoising, feature extraction, anomaly detection, and generative modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3135915",
   "metadata": {},
   "source": [
    "### 2. Suppose you want to train a classifier, and you have plenty of unlabeled training data but only a few thousand labeled instances. How can autoencoders help? How would you proceed?\n",
    "Autoencoders can help by pretraining the model on the unlabeled data and fine-tuning it on the labeled data. This can improve the model's performance and reduce overfitting. The autoencoder would be trained on the unlabeled data, and then its encoder would be used as a feature extractor for the classifier, which would be trained on the labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a791595",
   "metadata": {},
   "source": [
    "### 3. If an autoencoder perfectly reconstructs the inputs, is it necessarily a good autoencoder? How can you evaluate the performance of an autoencoder?\n",
    "Not necessarily. An autoencoder can perfectly reconstruct the inputs but fail to learn meaningful features or remove noise. Performance can be evaluated by comparing the input and output, calculating reconstruction loss, and using the encoder's features for downstream tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8193c291",
   "metadata": {},
   "source": [
    "### 4. What are undercomplete and overcomplete autoencoders? What is the main risk of an excessively undercomplete autoencoder? What about the main risk of an overcomplete autoencoder?\n",
    "Undercomplete autoencoders have a lower number of hidden units than the input or output dimension, which forces them to learn a compressed representation. The main risk is that the encoder can learn trivial solutions, such as copying the inputs to the output. Overcomplete autoencoders have more hidden units than the input or output dimension, which allows them to learn complex representations. The main risk is overfitting and learning trivial or redundant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f13d88",
   "metadata": {},
   "source": [
    "### 5. How do you tie weights in a stacked autoencoder? What is the point of doing so?\n",
    "Tying weights means sharing the weights of the encoder and decoder layers in a stacked autoencoder. The point is to reduce the number of parameters and prevent overfitting. This forces the decoder to reconstruct the input from the same compressed representation learned by the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23727acc",
   "metadata": {},
   "source": [
    "### 6. What is a generative model? Can you name a type of generative autoencoder?\n",
    "A generative model is a type of model that learns to generate new examples that are similar to the training data. A variational autoencoder (VAE) is a type of generative autoencoder that uses a probabilistic approach to generate new examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e21278",
   "metadata": {},
   "source": [
    "### 7. What is a GAN? Can you name a few tasks where GANs can shine?\n",
    "A GAN (Generative Adversarial Network) is a type of generative model that consists of two neural networks: a generator that learns to generate new examples, and a discriminator that learns to distinguish between the generated and real examples. GANs can be used for tasks such as image synthesis, image-to-image translation, and style transfer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aa04e7",
   "metadata": {},
   "source": [
    "### 8. What are the main difficulties when training GANs?\n",
    "The main difficulties when training GANs are instability, mode collapse, and vanishing gradients. To handle these issues, techniques such as Wasserstein GANs, gradient penalty, and progressive growing can be used. Proper tuning of hyperparameters and careful monitoring of the training process are also crucial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

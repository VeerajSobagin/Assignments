{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a249100d",
   "metadata": {},
   "source": [
    "### 1. Is it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n",
    "It is generally not okay to initialize all the weights to the same value, even if that value is selected randomly using He initialization, because this can lead to symmetry in the network and the weights may get stuck during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f695b8",
   "metadata": {},
   "source": [
    "### 2. Is it okay to initialize the bias terms to 0?\n",
    "It is okay to initialize the bias terms to 0, as long as the weights are initialized properly, because the bias terms serve as an offset and are not involved in the symmetry-breaking problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2030f6e0",
   "metadata": {},
   "source": [
    "### 3. Name three advantages of the ELU activation function over ReLU.\n",
    "Three advantages of the ELU activation function over ReLU are: 1) it can produce negative outputs without being stuck at 0, 2) it has smoother derivatives, which can improve optimization in deep networks, and 3) it can improve the model's accuracy by reducing the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6190b8c8",
   "metadata": {},
   "source": [
    "### 4. In which cases would you want to use each of the following activation functions: ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
    "ELU can be useful for deep neural networks, leaky ReLU can be useful when ReLU causes \"dying neurons\", ReLU is a good default choice, tanh and logistic can be useful in the output layer of binary classifiers, and softmax is useful for multiclass classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f200d31",
   "metadata": {},
   "source": [
    "### 5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a MomentumOptimizer?\n",
    "If the momentum hyperparameter is set too close to 1 when using a MomentumOptimizer, the optimizer may overshoot the minimum and fail to converge, resulting in oscillations or divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0da4ef",
   "metadata": {},
   "source": [
    "### 6. Name three ways you can produce a sparse model.\n",
    "Three ways to produce a sparse model are: 1) L1 regularization, which encourages weights to be exactly 0, 2) dropout, which randomly sets some neurons to 0 during training, and 3) weight sharing, which constrains multiple weights to be equal to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d93956f",
   "metadata": {},
   "source": [
    "### 7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)?\n",
    "Dropout can slow down training because it adds computational overhead and requires more iterations to converge, but it can also prevent overfitting and improve the model's accuracy. Dropout does not slow down inference, because it only affects the model during training and is not used during inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

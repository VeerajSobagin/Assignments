{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10a8e9eb",
   "metadata": {},
   "source": [
    "### 1. Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n",
    "\n",
    "Initializing all weights to the same value, even if selected randomly using He initialization, is not recommended because it may cause the neurons to learn the same features, leading to redundancy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ee122e",
   "metadata": {},
   "source": [
    "### 2. Is it OK to initialize the bias terms to 0?\n",
    "It is generally safe to initialize bias terms to 0 since they are typically small and do not significantly affect the initial network output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfa2e39",
   "metadata": {},
   "source": [
    "### 3. Name three advantages of the SELU activation function over ReLU.\n",
    "Three advantages of the SELU activation function over ReLU are that it avoids the dying ReLU problem, it can self-normalize activations, and it allows for deep neural networks without the need for other regularization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ee6124",
   "metadata": {},
   "source": [
    "### 4. In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
    "SELU is preferred when building deep neural networks. Leaky ReLU is useful when avoiding the dying ReLU problem. ReLU is widely used for hidden layers, and tanh and logistic are useful for output layers. Softmax is useful for multiclass classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bcaa5a",
   "metadata": {},
   "source": [
    "### 5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?\n",
    "If the momentum hyperparameter is set too close to 1, the optimizer may overshoot the minimum and oscillate around it, slowing down convergence or even preventing it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c957409",
   "metadata": {},
   "source": [
    "### 6. Name three ways you can produce a sparse model.\n",
    "Three ways to produce a sparse model are by using L1 regularization, by setting small weights to 0, and by using a sparsity constraint during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad9f0aa",
   "metadata": {},
   "source": [
    "### 7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?\n",
    "Dropout can slow down training because it introduces noise and requires more iterations to reach convergence. However, it can speed up inference since the dropout is turned off. MC Dropout can also slow down inference because it requires multiple forward passes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cbc450",
   "metadata": {},
   "source": [
    "### 8. Practice training a deep neural network on the CIFAR10 image dataset:\n",
    "    a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function.\n",
    "    b. Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_​data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters.\n",
    "    c. Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?\n",
    "    d. Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).\n",
    "    e. Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

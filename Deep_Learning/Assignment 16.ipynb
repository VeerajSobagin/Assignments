{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "614b6a76",
   "metadata": {},
   "source": [
    "### 1. Explain the Activation Functions in your own language\n",
    "    a) sigmoid\n",
    "    b) tanh\n",
    "    c) ReLU\n",
    "    d) ELU\n",
    "    e) LeakyReLU\n",
    "    f) swish\n",
    "\n",
    "a) The sigmoid activation function maps any input value to a value between 0 and 1, which makes it useful for binary classification problems. However, it can cause the vanishing gradient problem and is not recommended for deep neural networks.\n",
    "\n",
    "b) The tanh activation function maps any input value to a value between -1 and 1, which can help alleviate the vanishing gradient problem compared to sigmoid. However, it can still suffer from the same issue and is not commonly used in deep neural networks.\n",
    "\n",
    "c) The ReLU (Rectified Linear Unit) activation function returns 0 for any negative input and the input value for any positive input, which makes it computationally efficient and effective for deep neural networks. However, it can suffer from the \"dying ReLU\" problem, where some neurons may become inactive and stop learning.\n",
    "\n",
    "d) The ELU (Exponential Linear Unit) activation function is similar to ReLU for positive inputs, but smoothly transitions to a negative value for negative inputs. It can help alleviate the dying ReLU problem and has been shown to improve the accuracy of deep neural networks.\n",
    "\n",
    "e) The LeakyReLU activation function is similar to ReLU for positive inputs, but returns a small negative value for negative inputs. This can help prevent the dying ReLU problem and improve the accuracy of deep neural networks.\n",
    "\n",
    "f) The swish activation function is a recently proposed function that applies a sigmoid-like function to the input multiplied by the input value. It has shown promising results in some experiments and is being studied as a potential replacement for ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b0396a",
   "metadata": {},
   "source": [
    "### 2. What happens when you increase or decrease the optimizer learning rate?\n",
    "Increasing the optimizer learning rate can result in faster convergence during training, but can also cause the model to overshoot the optimal solution and diverge. Decreasing the learning rate can slow down convergence but can lead to better optimization and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21b137d",
   "metadata": {},
   "source": [
    "### 3. What happens when you increase the number of internal hidden neurons?\n",
    "Increasing the number of internal hidden neurons can allow the model to capture more complex patterns in the data, but can also lead to overfitting and slower training times. Finding the optimal number of neurons typically requires experimentation and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4dd0a8",
   "metadata": {},
   "source": [
    "### 4. What happens when you increase the size of batch computation?\n",
    "Increasing the size of batch computation can improve training efficiency and reduce noise in the parameter updates, but can also result in slower convergence and may require more memory to store intermediate computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8092cbc8",
   "metadata": {},
   "source": [
    "### 5. Why we adopt regularization to avoid overfitting?\n",
    "Regularization is used to prevent overfitting by adding constraints to the model parameters or by adding penalties to the loss function. This helps the model generalize better to new data and prevent memorization of the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ceff06",
   "metadata": {},
   "source": [
    "### 6. What are loss and cost functions in deep learning?\n",
    "In deep learning, the loss function measures how well the model is performing on the training data by comparing the predicted output to the actual output. The cost function is the average loss over the entire training set, and is used to optimize the model parameters during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0507f4f",
   "metadata": {},
   "source": [
    "### 7. What do ou mean by underfitting in neural networks?\n",
    "Underfitting in neural networks occurs when the model is too simple to capture the underlying patterns in the data, resulting in poor performance on both the training and validation sets. This can happen when the model has too few hidden neurons or is not trained for long enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b49e45",
   "metadata": {},
   "source": [
    "### 8. Why we use Dropout in Neural Networks?\n",
    "Dropout is used in neural networks as a regularization technique to prevent overfitting. It randomly drops out a fraction of the neurons during each training iteration, forcing the network to learn more robust features and reducing the risk of memorization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

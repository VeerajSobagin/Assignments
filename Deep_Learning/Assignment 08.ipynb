{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb9e66f7",
   "metadata": {},
   "source": [
    "### 1. What are the pros and cons of using a stateful RNN versus a stateless RNN?\n",
    "A stateful RNN is good for processing long sequences and preserving context across batches, but it is more difficult to parallelize and prone to overfitting. A stateless RNN is simpler to implement, parallelize, and less prone to overfitting, but it cannot preserve context across batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab844a2",
   "metadata": {},
   "source": [
    "### 2. Why do people use Encoderâ€“Decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?\n",
    "Encoder-Decoder RNNs are used for automatic translation because they can handle variable-length input and output sequences, and the encoder can capture the meaning of the input sequence and the decoder can generate a target sequence based on that meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ce3db4",
   "metadata": {},
   "source": [
    "### 3. How can you deal with variable-length input sequences? What about variable-length output sequences?\n",
    "To deal with variable-length input sequences, you can use padding, masking, or bucketing. To handle variable-length output sequences, you can use teacher forcing, scheduled sampling, or beam search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a19c09",
   "metadata": {},
   "source": [
    "### 4. What is beam search and why would you use it? What tool can you use to implement it?\n",
    "Beam search is a decoding algorithm that finds the most likely output sequence given a trained model. It considers multiple hypotheses in parallel and keeps the top-K most likely hypotheses at each step. The Python library, NLTK, provides an implementation of beam search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a2f827",
   "metadata": {},
   "source": [
    "### 5. What is an attention mechanism? How does it help?\n",
    "An attention mechanism is a way for a model to selectively focus on different parts of an input sequence when generating an output. It helps the model to better understand the context and relationships between different parts of the input sequence, resulting in better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e947a9",
   "metadata": {},
   "source": [
    "### 6. What is the most important layer in the Transformer architecture? What is its purpose?\n",
    "The most important layer in the Transformer architecture is the self-attention layer. It allows the model to attend to different parts of the input sequence and capture long-term dependencies without the need for recurrence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a93389",
   "metadata": {},
   "source": [
    "### 7. When would you need to use sampled softmax?\n",
    "Sampled softmax is used when there is a large number of output classes, making it impractical to compute the softmax function over all classes at each timestep. It randomly samples a subset of the output classes and computes the softmax over that subset, resulting in faster and more efficient training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
